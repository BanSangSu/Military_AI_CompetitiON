{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import numpy as np\n",
    "import shutil, random, os, sys, torch\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prj_dir = os.path.dirname(os.path.abspath(__file__)) # for script\n",
    "prj_dir = os.path.dirname(os.path.abspath(\"\")) # for jupyter\n",
    "sys.path.append(prj_dir)\n",
    "\n",
    "from modules.utils import load_yaml, get_logger\n",
    "from modules.metrics import get_metric_function\n",
    "from modules.earlystoppers import EarlyStopper\n",
    "from modules.losses import get_loss_function\n",
    "from modules.optimizers import get_optimizer\n",
    "from modules.schedulers import get_scheduler\n",
    "from modules.scalers import get_image_scaler\n",
    "from modules.datasets import SegDataset\n",
    "from modules.recorders import Recorder\n",
    "from modules.trainer import Trainer\n",
    "from models.utils import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_dir = os.path.dirname(os.path.abspath(\"baseline\")) # for jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Dev\\\\2022\\\\maicon\\\\baseline'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prj_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = 'train.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config_path = os.path.join(prj_dir, 'config', yaml)\n",
    "config = load_yaml(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Dev\\2022\\maicon\\baseline\\trian.ipynb ì…€ 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Set train serial: ex) 20211004\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_serial \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_serial \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdebug\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mdebug\u001b[39m\u001b[39m'\u001b[39m] \u001b[39melse\u001b[39;00m train_serial\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Set random seed, deterministic\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mmanual_seed(config[\u001b[39m'\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set train serial: ex) 20211004\n",
    "train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_serial = 'debug' if config['debug'] else train_serial\n",
    "\n",
    "# Set random seed, deterministic\n",
    "torch.cuda.manual_seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device(GPU/CPU)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(config['gpu_num'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create train result directory and set logger\n",
    "train_result_dir = os.path.join(prj_dir, 'results', 'train', train_serial)\n",
    "os.makedirs(train_result_dir, exist_ok=True)\n",
    "\n",
    "# Set logger\n",
    "logging_level = 'debug' if config['verbose'] else 'info'\n",
    "logger = get_logger(name='train',\n",
    "                    file_path=os.path.join(train_result_dir, 'train.log'),\n",
    "                    level=logging_level)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "train_dirs = os.path.join(prj_dir, 'data', 'train')\n",
    "\n",
    "# Load data and create dataset for train \n",
    "# Load image scaler\n",
    "train_img_paths = glob(os.path.join(train_dirs, 'x', '*.png'))\n",
    "train_img_paths, val_img_paths = train_test_split(train_img_paths, test_size=config['val_size'], random_state=config['seed'], shuffle=True)\n",
    "\n",
    "train_dataset = SegDataset(paths=train_img_paths,\n",
    "                        input_size=[config['input_width'], config['input_height']],\n",
    "                        scaler=get_image_scaler(config['scaler']),\n",
    "                        logger=None)\n",
    "val_dataset = SegDataset(paths=val_img_paths,\n",
    "                        input_size=[config['input_width'], config['input_height']],\n",
    "                        scaler=get_image_scaler(config['scaler']),\n",
    "                        logger=None)\n",
    "# Create data loader\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            num_workers=config['num_workers'], \n",
    "                            shuffle=config['shuffle'],\n",
    "                            drop_last=config['drop_last'])\n",
    "                            \n",
    "val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            num_workers=config['num_workers'], \n",
    "                            shuffle=False,\n",
    "                            drop_last=config['drop_last'])\n",
    "\n",
    "# logger.info(f\"Load dataset, train: {len(train_dataset)}, val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Opimizer, Scheduler, Loss and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = get_model(model_str=config['architecture'])\n",
    "model = model(classes=config['n_classes'],\n",
    "            encoder_name=config['encoder'],\n",
    "            encoder_weights=config['encoder_weight'],\n",
    "            activation=config['activation']).to(device)\n",
    "logger.info(f\"Load model architecture: {config['architecture']}\")\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = get_optimizer(optimizer_str=config['optimizer']['name'])\n",
    "optimizer = optimizer(model.parameters(), **config['optimizer']['args'])\n",
    "\n",
    "# Set Scheduler\n",
    "scheduler = get_scheduler(scheduler_str=config['scheduler']['name'])\n",
    "scheduler = scheduler(optimizer=optimizer, **config['scheduler']['args'])\n",
    "\n",
    "# Set loss function\n",
    "loss_func = get_loss_function(loss_function_str=config['loss']['name'])\n",
    "loss_func = loss_func(**config['loss']['args'])\n",
    "\n",
    "# Set metric\n",
    "metric_funcs = {metric_name:get_metric_function(metric_name) for metric_name in config['metrics']}\n",
    "logger.info(f\"Load optimizer:{config['optimizer']['name']}, scheduler: {config['scheduler']['name']}, loss: {config['loss']['name']}, metric: {config['metrics']}\")\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(model=model,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                loss_func=loss_func,\n",
    "                metric_funcs=metric_funcs,\n",
    "                device=device,\n",
    "                logger=logger)\n",
    "logger.info(f\"Load trainer\")\n",
    "\n",
    "# Set early stopper\n",
    "early_stopper = EarlyStopper(patience=config['earlystopping_patience'],\n",
    "                            logger=logger)\n",
    "# Set recorder\n",
    "recorder = Recorder(record_dir=train_result_dir,\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    logger=logger)\n",
    "logger.info(\"Load early stopper, recorder\")\n",
    "\n",
    "# Recorder - save train config\n",
    "shutil.copy(config_path, os.path.join(recorder.record_dir, yaml))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(prj_dir, 'config', \"train\"+\".yaml\")\n",
    "config = load_yaml(config_path)\n",
    "model = get_model(model_str=config['architecture'])\n",
    "model = model(classes=config['n_classes'],\n",
    "            encoder_name=config['encoder'],\n",
    "            encoder_weights=config['encoder_weight'],\n",
    "            activation=config['activation'])\n",
    "check_point_path = os.path.join(prj_dir, 'results', 'train','20221113_010847', 'model.pt')\n",
    "check_point = torch.load(check_point_path)\n",
    "model.load_state_dict(check_point['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "device_ids can only be None or contain a single element.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Dev\\2022\\maicon\\baseline\\trian.ipynb ì…€ 15\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#X15sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mif\u001b[39;00m NGPU \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#X15sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMulti\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#X15sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mparallel\u001b[39m.\u001b[39;49mDistributedDataParallel(model, device_ids\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(NGPU)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#X15sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m torch\u001b[39m.\u001b[39mmultiprocessing\u001b[39m.\u001b[39mset_start_method(\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Dev/2022/maicon/baseline/trian.ipynb#X15sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\nn\\parallel\\distributed.py:547\u001b[0m, in \u001b[0;36mDistributedDataParallel.__init__\u001b[1;34m(self, module, device_ids, output_device, dim, broadcast_buffers, process_group, bucket_cap_mb, find_unused_parameters, check_reduction, gradient_as_bucket_view, static_graph)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_and_throw(\n\u001b[0;32m    541\u001b[0m         \u001b[39mRuntimeError\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDistributedDataParallel is not needed when a module \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have any parameter that requires a gradient.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    546\u001b[0m \u001b[39mif\u001b[39;00m device_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(device_ids) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 547\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_and_throw(\n\u001b[0;32m    548\u001b[0m         \u001b[39mValueError\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mdevice_ids can only be None or contain a single element.\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    551\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_device_module \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m({p\u001b[39m.\u001b[39mdevice \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mparameters()}) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    552\u001b[0m distinct_device_types \u001b[39m=\u001b[39m {p\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39mparameters()}\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\py310_pytorch\\lib\\site-packages\\torch\\nn\\parallel\\distributed.py:674\u001b[0m, in \u001b[0;36mDistributedDataParallel._log_and_throw\u001b[1;34m(self, err_type, err_msg)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    673\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mset_error_and_log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(err_type)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00merr_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 674\u001b[0m \u001b[39mraise\u001b[39;00m err_type(err_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: device_ids can only be None or contain a single element."
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "# architectures = ['DeepLabV3Plus']\n",
    "# for architecture in architectures:\n",
    "#----\n",
    "# Load config\n",
    "# config_path = os.path.join(prj_dir, 'config', \"train\"+\".yaml\")\n",
    "# config = load_yaml(config_path)\n",
    "\n",
    "# Set train serial: ex) 20211004\n",
    "train_serial = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_serial = 'debug' if config['debug'] else train_serial\n",
    "\n",
    "# Set random seed, deterministic\n",
    "torch.cuda.manual_seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device(GPU/CPU)\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = str(config['gpu_num'])\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create train result directory and set logger\n",
    "# train_result_dir = os.path.join(prj_dir, 'results', 'train', architecture, train_serial)\n",
    "train_result_dir = os.path.join(prj_dir, 'results', 'train', train_serial)\n",
    "os.makedirs(train_result_dir, exist_ok=True)\n",
    "\n",
    "# Set logger\n",
    "logging_level = 'debug' if config['verbose'] else 'info'\n",
    "logger = get_logger(name='train',\n",
    "                    file_path=os.path.join(train_result_dir, 'train.log'),\n",
    "                    level=logging_level)\n",
    "\n",
    "#----\n",
    "# Load model\n",
    "# model = get_model(model_str=config['architecture'])\n",
    "# model = model(classes=config['n_classes'],\n",
    "#             encoder_name=config['encoder'],\n",
    "#             encoder_weights=config['encoder_weight'],\n",
    "#             activation=config['activation'])\n",
    "# check_point_path = os.path.join(prj_dir, 'results', 'train', 'DeepLabV3Plus','20221112_222652', 'model.pt')\n",
    "# check_point = torch.load(check_point_path)\n",
    "# model.load_state_dict(check_point['model'])\n",
    "# config_path = os.path.join(prj_dir, 'config', \"train\"+\".yaml\")\n",
    "# config = load_yaml(config_path)\n",
    "model = get_model(model_str=config['architecture'])\n",
    "model = model(classes=config['n_classes'],\n",
    "            encoder_name=config['encoder'],\n",
    "            encoder_weights=config['encoder_weight'],\n",
    "            activation=config['activation'])\n",
    "check_point_path = os.path.join(prj_dir, 'results', 'train', 'DeepLabV3Plus','20221112_222652', 'model.pt')\n",
    "check_point = torch.load(check_point_path)\n",
    "\n",
    "model_dict = check_point['model']\n",
    "keys = model_dict.keys()\n",
    "values = model_dict.values()\n",
    "new_keys = []\n",
    "\n",
    "for key in keys:\n",
    "  new_key = key[7:]    # remove the 'module.'\n",
    "  new_keys.append(new_key)\n",
    "new_dict = OrderedDict(list(zip(new_keys, values)))\n",
    "model.load_state_dict(new_dict)\n",
    "# model.load_state_dict(check_point['model'])\n",
    "logger.info(f\"Load model weight, {check_point_path}\")\n",
    "\n",
    "NGPU = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if NGPU > 1:\n",
    "    print(\"Multi\")\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(NGPU)))\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "model.to(device)\n",
    "logger.info(f\"Load model architecture: {config['architecture']}\")\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = get_optimizer(optimizer_str=config['optimizer']['name'])\n",
    "optimizer = optimizer(model.parameters(), **config['optimizer']['args'])\n",
    "\n",
    "# Set Scheduler\n",
    "scheduler = get_scheduler(scheduler_str=config['scheduler']['name'])\n",
    "scheduler = scheduler(optimizer=optimizer, **config['scheduler']['args'])\n",
    "\n",
    "# Set loss function\n",
    "loss_func = get_loss_function(loss_function_str=config['loss']['name'])\n",
    "loss_func = loss_func(**config['loss']['args'])\n",
    "\n",
    "# Set metric\n",
    "metric_funcs = {metric_name:get_metric_function(metric_name) for metric_name in config['metrics']}\n",
    "logger.info(f\"Load optimizer:{config['optimizer']['name']}, scheduler: {config['scheduler']['name']}, loss: {config['loss']['name']}, metric: {config['metrics']}\")\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(model=model,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                loss_func=loss_func,\n",
    "                metric_funcs=metric_funcs,\n",
    "                device=device,\n",
    "                logger=logger)\n",
    "logger.info(f\"Load trainer\")\n",
    "\n",
    "# Set early stopper\n",
    "early_stopper = EarlyStopper(patience=config['earlystopping_patience'],\n",
    "                            logger=logger)\n",
    "# Set recorder\n",
    "recorder = Recorder(record_dir=train_result_dir,\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    logger=logger)\n",
    "logger.info(\"Load early stopper, recorder\")\n",
    "\n",
    "# Recorder - save train config\n",
    "shutil.copy(config_path, os.path.join(recorder.record_dir, yaml))\n",
    "\n",
    "#----\n",
    "# Train\n",
    "print(\"START TRAINING\")\n",
    "logger.info(\"START TRAINING\")\n",
    "for epoch_id in range(config['n_epochs']):\n",
    "    \n",
    "    # Initiate result row\n",
    "    row = dict()\n",
    "    row['epoch_id'] = epoch_id\n",
    "    row['train_serial'] = train_serial\n",
    "    row['lr'] = trainer.scheduler.get_last_lr()\n",
    "\n",
    "    # Train\n",
    "    print(f\"Epoch {epoch_id}/{config['n_epochs']} Train..\")\n",
    "    logger.info(f\"Epoch {epoch_id}/{config['n_epochs']} Train..\")\n",
    "    tic = time()\n",
    "    trainer.train(dataloader=train_dataloader, epoch_index=epoch_id)\n",
    "    toc = time()\n",
    "    # Write tarin result to result row\n",
    "    row['train_loss'] = trainer.loss  # Loss\n",
    "    for metric_name, metric_score in trainer.scores.items():\n",
    "        row[f'train_{metric_name}'] = metric_score\n",
    "\n",
    "    row['train_elapsed_time'] = round(toc-tic, 1)\n",
    "    # Clear\n",
    "    trainer.clear_history()\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Epoch {epoch_id}/{config['n_epochs']} Validation..\")\n",
    "    logger.info(f\"Epoch {epoch_id}/{config['n_epochs']} Validation..\")\n",
    "    tic = time()\n",
    "    trainer.validate(dataloader=val_dataloader, epoch_index=epoch_id)\n",
    "    toc = time()\n",
    "    row['val_loss'] = trainer.loss\n",
    "    # row[f\"val_{config['metric']}\"] = trainer.score\n",
    "    for metric_name, metric_score in trainer.scores.items():\n",
    "        row[f'val_{metric_name}'] = metric_score\n",
    "    row['val_elapsed_time'] = round(toc-tic, 1)\n",
    "    trainer.clear_history()\n",
    "\n",
    "    # Performance record - row\n",
    "    recorder.add_row(row)\n",
    "    \n",
    "    # Performance record - plot\n",
    "    recorder.save_plot(config['plot'])\n",
    "\n",
    "    # Check early stopping\n",
    "    early_stopper.check_early_stopping(row[config['earlystopping_target']])\n",
    "    if early_stopper.patience_counter == 0:\n",
    "        recorder.save_weight(epoch=epoch_id)\n",
    "        \n",
    "    if early_stopper.stop:\n",
    "        print(f\"Epoch {epoch_id}/{config['n_epochs']}, Stopped counter {early_stopper.patience_counter}/{config['earlystopping_patience']}\")\n",
    "        logger.info(f\"Epoch {epoch_id}/{config['n_epochs']}, Stopped counter {early_stopper.patience_counter}/{config['earlystopping_patience']}\")\n",
    "        break\n",
    "\n",
    "print(\"END TRAINING\")\n",
    "logger.info(\"END TRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [05:50<00:00,  4.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random, os, sys, torch, cv2, warnings\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    " # Create train result directory and set logger\n",
    "pred_result_dir = os.path.join(prj_dir, 'results', 'pred', 'pred_serial')\n",
    "pred_result_dir_mask = os.path.join(prj_dir, 'results', 'pred', 'pred_serial', 'mask')\n",
    "os.makedirs(pred_result_dir, exist_ok=True)\n",
    "os.makedirs(pred_result_dir_mask, exist_ok=True)\n",
    "\n",
    "# Set logger\n",
    "logging_level = 'debug' if config['verbose'] else 'info'\n",
    "logger = get_logger(name='train',\n",
    "                    file_path=os.path.join(pred_result_dir, 'pred.log'),\n",
    "                    level=logging_level)\n",
    "\n",
    "# Set data directory\n",
    "test_dirs = os.path.join(prj_dir, 'data', 'test')\n",
    "test_img_paths = glob(os.path.join(test_dirs, 'x', '*.png'))\n",
    "\n",
    "#! Load data & create dataset for train \n",
    "test_dataset = SegDataset(paths=test_img_paths,\n",
    "                        input_size=[config['input_width'], config['input_height']],\n",
    "                        scaler=get_image_scaler(config['scaler']),\n",
    "                        mode='test',\n",
    "                        logger=logger)\n",
    "\n",
    "# Create data loader\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            num_workers=config['num_workers'],\n",
    "                            shuffle=False,\n",
    "                            drop_last=False)\n",
    "\n",
    "# Predict\n",
    "logger.info(f\"START PREDICTION\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_id, (x, orig_size, filename) in enumerate(tqdm(test_dataloader)):\n",
    "        \n",
    "        x = x.to(device, dtype=torch.float)\n",
    "        y_pred = model(x)\n",
    "        y_pred_argmax = y_pred.argmax(1).cpu().numpy().astype(np.uint8)\n",
    "        orig_size = [(orig_size[0].tolist()[i], orig_size[1].tolist()[i]) for i in range(len(orig_size[0]))]\n",
    "        # Save predict result\n",
    "        for filename_, orig_size_, y_pred_ in zip(filename, orig_size, y_pred_argmax):\n",
    "            resized_img = cv2.resize(y_pred_, [orig_size_[1], orig_size_[0]], interpolation=cv2.INTER_NEAREST)\n",
    "            cv2.imwrite(os.path.join(pred_result_dir_mask, filename_), resized_img)\n",
    "logger.info(f\"END PREDICTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4755a0ac336b2d92cae304601054584f3e51fa9cdaefb452a7f1602e63829cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
